{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "22Osd5PRPx6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Neem_2_vEIH5"
      },
      "outputs": [],
      "source": [
        "# Introduction to Neural Networks (CSE 40868/60868)\n",
        "# University of Notre Dame, Spring 2023\n",
        "# Final Project Portion 3: Multi Layer Perceptron (MLP) for Genre Classification\n",
        "# Based upon MLP used in Practical 1 (Thomas Summe, Zheng Ning, Adam Czajka, February 2023)\n",
        "# _________________________________________________________________________\n",
        "# Christine Van Kirk, Mia Manabat, Camille Knott (April 2023)\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Build PyTorch Dataset for Genre Data"
      ],
      "metadata": {
        "id": "P44iSjkiP7B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset class\n",
        "class GenreData(Dataset):\n",
        "    \n",
        "    # constructor for Pytorch dataset class\n",
        "    def __init__(self, path):\n",
        "\n",
        "        # read dataset from path\n",
        "        data = pd.read_csv(path, header=0)\n",
        "        \n",
        "        # instantiate label encoder\n",
        "        le = LabelEncoder()\n",
        "\n",
        "        # numberize the attributed features\n",
        "        for col in data.columns:\n",
        "            data[col] = le.fit(data[col]).transform(data[col])\n",
        "        self.data = data\n",
        "\n",
        "    # returns the number of samples in our dataset\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # loads and returns a sample from the dataset at the given index \"item\"\n",
        "    def __getitem__(self, item):\n",
        "        x = self.data.drop(['genre'], axis=1).values\n",
        "        x = torch.tensor(x).float()\n",
        "\n",
        "        # normalize the data\n",
        "        feat = (x/torch.max(x))[item, :]\n",
        "        y = self.data['genre'].values\n",
        "        label = torch.tensor(y).float().unsqueeze(1)[item, :]\n",
        "\n",
        "        return feat, label"
      ],
      "metadata": {
        "id": "AgShgroeHRCc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Specify Network-Related Hyper-Parameters"
      ],
      "metadata": {
        "id": "kCubZ2D6WN5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"ER_EchoNest_AudioFeatures.csv\" # specify location of Genre.csv\n",
        "input_dim = 8               # equal to number of features describing each Genre\n",
        "hidden_dim = 90             # number of hidden neurons\n",
        "output_dim = 1              # number of output neurons\n",
        "device = 'cpu'              # we will be using CPU in this practical\n",
        "batch_size = 200            # specify batch size"
      ],
      "metadata": {
        "id": "36idPZ5USUuE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Define Model Evaluation Function"
      ],
      "metadata": {
        "id": "nwAAUkYDaJ8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluates the trained model\n",
        "def evaluate(model, loader):\n",
        "\n",
        "    # we need to switch the model into the evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # create a list to store the prediction results\n",
        "    res_store = []\n",
        "    for batch in loader:\n",
        "        x, y = batch\n",
        "        \n",
        "        # make a prediction for a data sample \"x\"\n",
        "        pred = model(x)\n",
        "        pred = (pred > 0.5).float().squeeze(1)\n",
        "        y = y.squeeze(1)\n",
        "\n",
        "        # if the prediction is correct, append True; else append False\n",
        "        res_store += (pred == y).tolist()\n",
        "\n",
        "    # return the classification accuracy\n",
        "    acc = sum(res_store)/len(res_store)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "KwkEQn4xHpiJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Building The Multi-Layer Perceptron By Hand"
      ],
      "metadata": {
        "id": "oIG8nq9dV2pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-layer perceptron (MLP) model class\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    # constructor for the MLP model\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # Define the hidden layer with \"hidden_dim\" linear neurons \n",
        "        # and accepting inputs of size \"input_dim\"\n",
        "        # (Use \"torch.randn()\" Pytorch function to create a matrix of size input_dim x hidden_dim)\n",
        "        self.W1 = torch.randn(input_dim, hidden_dim)\n",
        "\n",
        "        # Define the ReLU activation function (we will use it in the hidden layer)\n",
        "        # (Define ReLU with torch.max and derivative of ReLU using torch.gt and keep constants in a tensor)\n",
        "        self.ReLU = lambda x : torch.max(x,torch.tensor(0))\n",
        "        ## self.ReLU = lambda x : torch.max(x)\n",
        "        self.ReLU_prime = lambda x : torch.gt(x, 0).float()\n",
        "      \n",
        "        # Define the output layer with \"output_dim\" linear neurons\n",
        "        # and accepting inputs of size \"hidden_dim\"\n",
        "        self.W2 = torch.randn(hidden_dim, output_dim)\n",
        "\n",
        "        # Finally , define the sigmoid activation function \n",
        "        # (Define sigmoid with torch.exp and derivative of sigmoid)\n",
        "        self.sigmoid = lambda x : 1 / (1 + torch.exp(-x))\n",
        "        self.sigmoid_prime = lambda x : self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "    # define the forward procedure for the network\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # Pass the input to the first layer\n",
        "        self.z1 = torch.matmul(x, self.W1)\n",
        "        \n",
        "        # Apply the activation function in this first layer\n",
        "        self.y1 = self.ReLU(self.z1)\n",
        "        \n",
        "        # Pass the output of the first layer to the next (output) layer\n",
        "        self.z2 = torch.matmul(self.y1, self.W2)\n",
        "\n",
        "        # Apply the activation function in the output layer\n",
        "        y_hat = self.sigmoid(self.z2)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    # define the backward procedure for the network\n",
        "    def backward(self, X, d_cost_d_y, y_hat):\n",
        "        \n",
        "        # d_cost_d_z2 = ... # requires sigmoid prime\n",
        "        d_cost_d_z2 = d_cost_d_y * self.sigmoid_prime(self.z2)\n",
        "\n",
        "        # d_cost_d_y1 = ... # requires torch.matmul\n",
        "        d_cost_d_y1 = torch.matmul(d_cost_d_z2, torch.t(self.W2))\n",
        "\n",
        "        # d_cost_d_z1 = ... # requires ReLU prime\n",
        "        d_cost_d_z1 = d_cost_d_y1 * self.ReLU_prime(self.z1)\n",
        "        \n",
        "        # d_cost_d_W1 = ... # requires torch.matmul\n",
        "        d_cost_d_W1 = torch.matmul(torch.t(d_cost_d_z1), X)\n",
        "\n",
        "        # d_cost_d_W2 = ... # requires torch.matmul\n",
        "        d_cost_d_W2 = torch.matmul(torch.t(self.y1), d_cost_d_z2)\n",
        "\n",
        "        self.W1 -= torch.t(d_cost_d_W1)\n",
        "        self.W2 -= d_cost_d_W2\n"
      ],
      "metadata": {
        "id": "SkzjTKI7HVrl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate model and dataset"
      ],
      "metadata": {
        "id": "P5yCN18SYQir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed the random number generator for all devices (both CPU and CUDA)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Instantiate the dataset\n",
        "data = GenreData(data_path)\n",
        "\n",
        "# Instantiate the MLP model: 22 features (input size), 90 neurons in the hidden layer, and 1 output neuron\n",
        "# (you may experiment with these numbers to see what happens!)\n",
        "mlp = MLP(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Here we use torch random_split() function to split the data into training set, validation set and test set \n",
        "# e.g., with the following proportions: 0.6 : 0.2 : 0.2; hint: len(data) will give you number of samples in our dataset\n",
        "# (see https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)\n",
        "train_set_size = 0.6\n",
        "val_set_size = 0.2\n",
        "test_set_size = 0.2\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(data,[train_set_size,val_set_size,test_set_size])\n",
        "\n",
        "# Wrap the dataset into Pytorch dataloader to pass samples in \"minibatches\"\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n"
      ],
      "metadata": {
        "id": "V844bi-iQOD7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Randomly-Initialized Network"
      ],
      "metadata": {
        "id": "VtrBiNAjYplJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = evaluate(mlp, test_dataloader)\n",
        "print(f\"Test accuracy = {acc}\")"
      ],
      "metadata": {
        "id": "U7e229tTnTUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd2f90d-7a4b-4010-834e-ab32599a3542"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 0.48375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Training The Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "D-98TuIKZ2cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify Training-Related Hyper-Parameters"
      ],
      "metadata": {
        "id": "zWrOdVczaAID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save = \"best_model\"\n",
        "epochs = 20\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "LTcmSqDgaAeM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Loss Function And Its Derivative"
      ],
      "metadata": {
        "id": "6XB3cXJpWiXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y_hat, y_star):\n",
        "    return torch.mean(torch.square(torch.sub(y_hat,y_star)))\n",
        "\n",
        "def MSE_prime(y_hat, y_star):\n",
        "    return 2 * torch.sub(y_hat, y_star)"
      ],
      "metadata": {
        "id": "6eoQKbA42GlD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Training Loop"
      ],
      "metadata": {
        "id": "avPeOZI3YY3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_best = 0.0\n",
        "for epoch in range(epochs):\n",
        "    mlp.train()\n",
        "    print(f\"epoch:{epoch}\")\n",
        "\n",
        "    # iterate batches in dataloader\n",
        "    for batch in train_dataloader:\n",
        "\n",
        "        x, y_star = batch\n",
        "\n",
        "        y = mlp(x)\n",
        "        cost = MSE(y,y_star) # Calculate cost\n",
        "        d_cost_d_y = MSE_prime(y,y_star)*learning_rate # calculate output gradient and multiply by learning rate\n",
        "\n",
        "        # performs a single optimization step (weights update)\n",
        "        mlp.backward(x,d_cost_d_y,y)\n",
        "\n",
        "    # evaluate the model \n",
        "    acc = evaluate(mlp, val_dataloader)\n",
        "\n",
        "    if acc > acc_best and save:\n",
        "        torch.save(mlp.W1, save + \"_W1\")\n",
        "        torch.save(mlp.W2, save + \"_W2\")\n",
        "    \n",
        "    # if (epoch+1) % 5 == 0: <- use this if you want to print the validation accuracy every 5 epochs\n",
        "    print(f\"Epoch: #{epoch+1}: validation accuracy = {acc*100:.2f}%; loss={cost}\")"
      ],
      "metadata": {
        "id": "3RGI4Jh5VqBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33552388-1049-4688-d516-8192d6b0920a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "Epoch: #1: validation accuracy = 49.88%; loss=0.5042392611503601\n",
            "epoch:1\n",
            "Epoch: #2: validation accuracy = 54.50%; loss=0.47956496477127075\n",
            "epoch:2\n",
            "Epoch: #3: validation accuracy = 76.12%; loss=0.23532545566558838\n",
            "epoch:3\n",
            "Epoch: #4: validation accuracy = 76.12%; loss=0.2285148948431015\n",
            "epoch:4\n",
            "Epoch: #5: validation accuracy = 73.88%; loss=0.17348459362983704\n",
            "epoch:5\n",
            "Epoch: #6: validation accuracy = 77.25%; loss=0.17446598410606384\n",
            "epoch:6\n",
            "Epoch: #7: validation accuracy = 76.50%; loss=0.19375532865524292\n",
            "epoch:7\n",
            "Epoch: #8: validation accuracy = 77.00%; loss=0.28695279359817505\n",
            "epoch:8\n",
            "Epoch: #9: validation accuracy = 77.12%; loss=0.237042635679245\n",
            "epoch:9\n",
            "Epoch: #10: validation accuracy = 77.12%; loss=0.1875089406967163\n",
            "epoch:10\n",
            "Epoch: #11: validation accuracy = 72.25%; loss=0.23906998336315155\n",
            "epoch:11\n",
            "Epoch: #12: validation accuracy = 76.75%; loss=0.24193742871284485\n",
            "epoch:12\n",
            "Epoch: #13: validation accuracy = 76.75%; loss=0.2220839560031891\n",
            "epoch:13\n",
            "Epoch: #14: validation accuracy = 76.00%; loss=0.1845902055501938\n",
            "epoch:14\n",
            "Epoch: #15: validation accuracy = 76.75%; loss=0.2430894523859024\n",
            "epoch:15\n",
            "Epoch: #16: validation accuracy = 76.88%; loss=0.22582542896270752\n",
            "epoch:16\n",
            "Epoch: #17: validation accuracy = 77.88%; loss=0.17991703748703003\n",
            "epoch:17\n",
            "Epoch: #18: validation accuracy = 78.00%; loss=0.1647418588399887\n",
            "epoch:18\n",
            "Epoch: #19: validation accuracy = 78.00%; loss=0.12328102439641953\n",
            "epoch:19\n",
            "Epoch: #20: validation accuracy = 76.38%; loss=0.1604819893836975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test The Model (with unknown test data)"
      ],
      "metadata": {
        "id": "TrW1v5ASauN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model’s weights\n",
        "mlp.W1 = torch.load(save + \"_W1\")\n",
        "mlp.W2 = torch.load(save + \"_W2\")\n",
        "acc = evaluate(mlp, test_dataloader)\n",
        "print(f\"Test accuracy = {acc}\")"
      ],
      "metadata": {
        "id": "6juSZhrDSu4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27053fd3-731b-4415-c60c-d21ebf670e73"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 0.7575\n"
          ]
        }
      ]
    }
  ]
}